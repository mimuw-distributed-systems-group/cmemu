use super::decode::{PipelineStepPack, TriggerData};
use super::instruction::{Instruction, MemoryInstructionDescription};
use super::lsu::ReadDataCallback;
use super::register_bank::{RegisterBitmap, RegisterID, XPSR};
use super::{CoreComponent, Decode, Fetch, LSU, RegisterBank};
use crate::common::{Address, BitstringUtils, Word};
use crate::component::core::decode::Brchstat;
use crate::component::core::execute::instruction::memory_instruction::MultipleLoadStoreExecutionState;
use crate::confeature::cm_hyp;
use crate::engine::{
    Context, DisableableComponent, Subcomponent, TickComponent, TickComponentExtra,
};
#[cfg(feature = "cycle-debug-logger")]
use crate::proxy::CycleDebugLoggerProxy;
use crate::utils::{DisplayOr, IfExpr, dife, ife};
use instruction::ExecutionStepResult;
use log::trace;
use owo_colors::OwoColorize;
#[cfg(debug_assertions)]
use std::cell::Cell;
use std::ops::BitAnd;
use strum::IntoStaticStr;

mod instruction;

/// Execute subcomponent runs "Operation" section of instructions decoded by Decode subcomponent.
///
/// Normally only one instruction is executed at a time.
/// The needed data (e.g. instruction, its address, internal state, "dirty" registers;
/// called `InstructionExecutionContext`) are stored in `main_slot` slot.
///
/// But sometimes the next instruction can be started before the currently executed one
/// is finished. This is mainly the case of LSU instructions (which "pipeline").
/// Also, the xMULL;MLA sequence pipelines when certain register dependency is fulfilled.
///
/// We say that the latter instruction was pipelined and its execution context
/// is stored in `pipelined_slot` slot
/// (`instruction_address(main_slot) < instruction_address(pipelined_slot)`)
///
/// When the instruction from `main_slot` finishes execution, the instruction execution context
/// from `pipelined_slot` slot is moved to `main_slot`.
#[derive(Subcomponent, TickComponent, DisableableComponent)]
#[subcomponent_1to1]
pub(super) struct Execute {
    pipelined_slot: Option<InstructionExecutionContext>,
    main_slot: Option<InstructionExecutionContext>,

    /// Stores the last finished instruction, in case some code needs to use a stale value
    last_ctx: Option<InstructionExecutionContext>,
    /// Were there any idle cycles previously?
    /// Some code may look at this to decide whether to use the stale ctx.
    idle_prev_cycles: u8,

    /// The same set of methods are used to execute instruction from `main_ctx`
    /// and `pipelined_ctx`.
    /// This field determines which `InstructionExecutionContext` should be used.
    active_slot: ActiveSlot,

    /// We cannot know that until tock, so this should be a Register (flop) actually
    execute_finished_prev_cycle: bool,

    /// Whether the most recent instruction that has been (or was) handled by execute, uses LSU
    most_recent_instruction_uses_lsu: bool,

    /// Address of currently being executed instruction
    /// or address of the next instruction if no instruction is being executed
    this_instr_addr: Word,
    next_instr_addr: Word,

    #[cfg(debug_assertions)]
    get_pc_called: Cell<bool>,

    interruption_state: InterruptStackingOrUnstackingState,
}

/// Contains all data needed to execute instruction.
struct InstructionExecutionContext {
    /// Contains decoded instructions, addresses and other information generated by decode
    pipeline_step_pack: PipelineStepPack,

    /// Internal data. Use this field in multicycle instruction execution
    /// when you want to make data calculated in first cycle available in later ones.
    state: InstructionExecutionState,

    /// Number of already taken cycles to execute current instruction.
    /// Counts only execution phase, starting from 0.
    cycle_cntr: u32,

    /// Stores information which registers will be written by the instruction
    /// in this or later cycle.
    dirty_regs: RegisterBitmap,
    next_dirty_regs: RegisterBitmap,

    /// Tracks which registers have been already modified by current execution.
    written_regs: RegisterBitmap,
    next_written_regs: RegisterBitmap,

    is_finished: bool,
    was_pipelined: bool,

    /// [`XPSR`] value visible for main instruction. Folded one sees result of the main one.
    visible_xpsr: XPSR,
    /// [`XPSR`] value visible after execution of this instruction (and optional folded one).
    /// Can be modified only in the very first cycle (`cycle_cntr == 0`).
    /// Note: IT instruction can only fold onto 1-cycle instructions. More specifically, it cannot
    ///       fold onto LSU operations, wide instruction and CPS instruction.
    next_xpsr: XPSR,

    // Assumptions: execute modifies APSR & EPSR, interrupt controller modifies IPSR
    #[cfg(debug_assertions)]
    apsr_modified: bool,
    #[cfg(debug_assertions)]
    epsr_modified: bool,
    #[cfg(debug_assertions)]
    folded_instr_executed: bool,
}

#[derive(Debug)]
enum InstructionExecutionState {
    None,

    /// State variant used by all multiplication instructions that writes the result
    /// to 2 registers.
    MultiplyLong(MultiplyLongExecutionState),

    /// State variant used by all load and store instructions that reads/writes
    /// at most one register.
    SingleLoadStore(SingleLoadStoreExecutionState),

    /// State variant used by all load and store instructions that can read or write
    /// two registers (e.g. LDRD/STRD). This variant can handle registers
    /// in an arbitary order while `MultipleLoadStore` can't.
    /// State variant used by all load and store instructions that can read or write
    /// multiple registers (e.g. PUSH/POP/LDM/STM).
    MultipleLoadStore(MultipleLoadStoreExecutionState),

    /// State variant used by the table branches (TBB, TBH) to store an offset
    /// from PC for branch in the next cycle.
    /// Note: Code is simpler without separate type for variant content.
    TableBranchOffset {
        branch_offset: Word,
        dest_addr: Option<Word>,
    },
}

#[derive(Debug, Clone, Copy)]
struct MultiplyLongExecutionState {
    result_hi: Word,
    result_lo: Word,
    write_hi_cycle_no: u32,
    write_lo_cycle_no: u32,
}

#[derive(Debug)]
struct SingleLoadStoreExecutionState {}

#[derive(Clone, Copy, Debug, Eq, PartialEq)]
enum ActiveSlot {
    None,
    Main,
    Pipelined,
}

/// Contains information if Execute should be interrupted for stacking/unstacking,
/// because one of interrupts has been risen.
#[derive(Debug, Eq, PartialEq, Clone, Copy)]
enum InterruptStackingOrUnstackingState {
    None,
    /// State from WFI
    WaitingForInterrupt,
    Requested,
    WaitingForExecutionToFinish,
    Running,
}

impl TickComponentExtra for Execute {
    #[cfg(debug_assertions)]
    fn tick_assertions(&self) {
        if let (Some(main_iectx), Some(pipelined_iectx)) = (&self.main_slot, &self.pipelined_slot) {
            assert!(
                (main_iectx.next_dirty_regs & pipelined_iectx.next_written_regs).are_all_cleared(),
                "Invalid order of setting registers."
            );
        }
        assert!(
            self.last_ctx.as_ref().is_none_or(|ctx| ctx.is_finished),
            "Last ctx contains an unfinished instruction!"
        );
    }

    fn tick_extra(&mut self) {
        #[cfg(debug_assertions)]
        {
            self.get_pc_called.set(false);
        }

        self.this_instr_addr = self.next_instr_addr;

        self.tick_slots();
    }
}

impl Execute {
    fn tick_slots(&mut self) {
        if self.main_slot.is_none() && self.pipelined_slot.is_none() {
            self.idle_prev_cycles += 1;
        }

        if let Some(main_ctx) = &mut self.main_slot {
            self.idle_prev_cycles = 0;
            if main_ctx.is_finished {
                self.last_ctx = self.main_slot.take();
            } else {
                main_ctx.cycle_cntr += 1;
                main_ctx.dirty_regs = main_ctx.next_dirty_regs;
                main_ctx.written_regs = main_ctx.next_written_regs;
            }
        }

        if let Some(pipelined_ctx) = &mut self.pipelined_slot {
            self.idle_prev_cycles = 0;
            // Pipelined instruction must wait with finishing
            // until the preceding "main" instruction finishes execution.
            if pipelined_ctx.is_finished && self.main_slot.is_none() {
                self.last_ctx = self.pipelined_slot.take();
            } else {
                pipelined_ctx.cycle_cntr += 1;
                pipelined_ctx.dirty_regs = pipelined_ctx.next_dirty_regs;
                pipelined_ctx.written_regs = pipelined_ctx.next_written_regs;
            }
        }

        if self.main_slot.is_none() {
            self.main_slot = self.pipelined_slot.take();
        }
        self.execute_finished_prev_cycle = self.main_slot.is_none();
    }

    /// Get the most recent instruction execution context: that is: the pipelined slot, the main slot, or the previous one.
    /// Returns None after instruction barriers (initial, vectors, isb.w)
    fn get_most_recent_iectx(&self) -> Option<&InstructionExecutionContext> {
        // TODO: make isb.w and vectors clean this!
        self.pipelined_slot
            .as_ref()
            .or(self.main_slot.as_ref())
            .or(self.last_ctx.as_ref())
    }

    pub(super) fn get_last_effective_instruction(core: &CoreComponent) -> Option<&Instruction> {
        Self::component_to_member(core)
            .get_most_recent_iectx()
            .map(|iectx| iectx.effective_instruction())
    }
}

// ============================================================================
// API
// ============================================================================

impl Execute {
    pub(super) fn new() -> Self {
        Self {
            pipelined_slot: None,
            main_slot: None,

            last_ctx: None,
            idle_prev_cycles: 0,

            active_slot: ActiveSlot::None,

            execute_finished_prev_cycle: false,
            most_recent_instruction_uses_lsu: false,

            this_instr_addr: Word::from(0),
            next_instr_addr: Word::from(0),

            #[cfg(debug_assertions)]
            get_pc_called: Cell::new(false),

            interruption_state: InterruptStackingOrUnstackingState::None,
        }
    }

    /// Updates interruption state. Especially might interrupt currently
    /// executing instructions or decide to wait until they are finished.
    /// Must be called before any other [`Execute`] method.
    /// Would be a part of `tick_extra`, however it might need to update
    /// other parts of [`CoreComponent`] state (e.g. [`XPSR`] or [`LSU`]).
    #[allow(clippy::shadow_unrelated)] // For shadowing `this`.
    pub(super) fn run_update_interruption_state(core: &mut CoreComponent) {
        let mut this = Self::get_proxy(core);
        match (this.main_slot.is_some(), this.interruption_state) {
            // Execution finished. Run stacking/unstacking if needed.
            (
                false,
                InterruptStackingOrUnstackingState::Requested
                | InterruptStackingOrUnstackingState::WaitingForExecutionToFinish,
            ) => {
                debug_assert!(this.pipelined_slot.is_none());
                this.last_ctx = None;
                this.idle_prev_cycles = 0;
                this.interruption_state = InterruptStackingOrUnstackingState::Running;
            }
            // Stacking/unstacking requested. Interrupt current execution if possible.
            (true, InterruptStackingOrUnstackingState::Requested) => {
                let next_state = if Self::interrupt_execution_if_possible(this.component_mut()) {
                    InterruptStackingOrUnstackingState::Running
                } else {
                    InterruptStackingOrUnstackingState::WaitingForExecutionToFinish
                };
                this.interruption_state = next_state;
            }
            _ => (),
        }
    }

    /// Returns true if `Execute` subcomponent is ready to start running `instruction`.
    pub(super) fn is_ready(
        core: &CoreComponent,
        #[allow(unused)] ctx: &mut Context,
        instruction: &Instruction,
        it_skipped: bool,
        has_folded_instruction: bool,
    ) -> bool {
        // TODO: we have some fundamental issues with decision ordering -
        //       deciding this should be done "at end of the cycle", while here we already updated
        //       most of the state
        let this = Self::component_to_member(core);

        if this.interruption_state != InterruptStackingOrUnstackingState::None {
            // Note: this may be also waiting for an interrupt
            return false;
        }

        if this.pipelined_slot.is_some() {
            debug_assert!(this.main_slot.is_some());
            return false;
        }

        if let Some(main_ctx) = &this.main_slot {
            debug_assert!(!main_ctx.is_finished);
            debug_assert!(this.pipelined_slot.is_none());
            let reason =
                Self::can_be_pipelined(core, instruction, it_skipped, has_folded_instruction);
            #[cfg(feature = "cycle-debug-logger")]
            CycleDebugLoggerProxy.on_free_static_str(ctx, "can_pipeline_is_ready", reason.into());
            reason == PipeliningResult::Pipelines
        } else {
            debug_assert!(this.main_slot.is_none());
            // nop/skipped use stale main_slot?
            true
        }
    }

    pub(super) fn was_last_cycle(core: &CoreComponent) -> bool {
        let this = Self::component_to_member(core);
        this.execute_finished_prev_cycle
    }

    pub(super) fn is_free(core: &CoreComponent) -> bool {
        let this = Self::component_to_member(core);
        this.main_slot.is_none() && this.pipelined_slot.is_none()
    }

    #[allow(clippy::shadow_unrelated)]
    pub(super) fn move_pipeline(core: &mut CoreComponent, pipeline_step_pack: PipelineStepPack) {
        trace!("Move pipeline: {:?}", pipeline_step_pack);
        let mut this = Self::get_proxy(core);

        #[cfg(debug_assertions)]
        {
            assert!(
                !this.get_pc_called.get(),
                "During this cycle, Execute::get_pc() has been called before Execute::move_pipeline(),
                thus value that had been returned by the Execute::get_pc() might be invalid."
            );
            assert_eq!(
                this.interruption_state,
                InterruptStackingOrUnstackingState::None
            );
            assert!(this.pipelined_slot.is_none());
        }

        let xpsr = match &this.main_slot {
            None => RegisterBank::get_xpsr(this.component()),
            Some(main_iectx) => {
                // Assumption used: `next_xpsr` is always calculated in the very first cycle, so we can
                //                  treat it as visible xPSR for the following instruction.
                main_iectx.next_xpsr
            }
        };

        this.this_instr_addr = pipeline_step_pack.address.into();
        this.next_instr_addr = this.this_instr_addr;
        let mut iectx = InstructionExecutionContext::new(pipeline_step_pack, xpsr);
        this.most_recent_instruction_uses_lsu = {
            let effective = if *cm_hyp::spec_fetch::SKIPPED_LSUCNT {
                iectx.raw_instruction()
            } else {
                iectx.instruction()
            };
            // See ``control_flow/add_pc.asm`` -- the stall after such a jump bumps LSUCNT, not CPI
            // Apparently, it uses a similar path to ``LDR pc`` and other LSU jumps (but no interworking)
            effective.is_lsu_instruction()
                || matches!(
                    effective,
                    Instruction::Add_Register {
                        rd: RegisterID::PC,
                        ..
                    } | Instruction::Add_SPPlusRegister {
                        rd: RegisterID::PC,
                        ..
                    }
                )
        };
        if this.main_slot.is_none() {
            this.main_slot = Some(iectx);
        } else {
            iectx.mark_pipelined();
            this.pipelined_slot = Some(iectx);
        }
    }

    pub(super) fn can_nop_think_it_is_multicycle(
        core: &CoreComponent,
        instruction: &Instruction,
        is_skipped: bool,
        has_folded_instruction: bool,
        #[allow(unused)] ctx: &mut Context,
    ) -> Option<PipeliningResult> {
        // Case of LDR + nop and speculative fetching next instruction
        // Nop behaves like a multi-cycle instruction
        // NOTE: for skipped => false: see tests with itxxx fail, ldr.n, nop.n, ... (e.g. definitive_it_basic.tzst).
        // This nop doesn't show the behaviour of postponing advancing head (multicycle).
        let this = Self::component_to_member(core);
        this.get_most_recent_iectx().map(|last_ctx| {
            let result = Self::can_lsu_instr_be_pipelined(
                core,
                last_ctx,
                instruction,
                is_skipped,
                has_folded_instruction,
                Some(ctx),
            );
            #[cfg(feature = "cycle-debug-logger")]
            CycleDebugLoggerProxy.on_free_static_str(ctx, "can_nop_think_early", result.into());

            // Some flags use stale values
            // ATM we simulate it returning this special reason
            match result {
                PipeliningResult::Late => PipeliningResult::Pipelines,
                x => x,
            }
        })
    }

    pub(super) fn postponing_in_exec(core: &CoreComponent) -> bool {
        use crate::confeature::cm_hyp::spec_fetch;
        // TODO: enough to check main slot? (i.e. lsu + nop)
        let this = Self::component_to_member(core);
        if let Some(ref iectx) = this.main_slot {
            let instr = iectx.raw_instruction();
            instr.does_postpone_advance_head()
                && ((instr.is_mul_div_instruction() && *spec_fetch::POSTPONING_SKIPPED_ALU)
                    || (instr.is_lsu_instruction() && *spec_fetch::POSTPONING_SKIPPED_LSU)
                    || (instr.is_branch() && *spec_fetch::POSTPONING_SKIPPED_BRANCH)
                    || (!iectx.visible_xpsr.in_it_block())
                    || (iectx.visible_xpsr.it_condition_passed()))
        } else {
            false
        }
    }

    pub(super) fn flags_on_critical_path_to_agu(core: &CoreComponent) -> bool {
        // TODO: enough to check main slot? mul + mla?
        // NOTE: misc/large_tests_extract/agu_reg_dep.asm shows that instruction which conditionally
        // don't set FLAG in IT, prevents forwarding to AGU
        let this = Self::component_to_member(core);
        if let Some(ref iectx) = this.main_slot {
            // TODO: What if skipped?
            let (setflags, could_setflags) = iectx.instruction().does_set_flags();
            setflags || could_setflags || iectx.has_folded_instruction()
        } else {
            false
        }
    }

    /// Increments DWT counters.
    /// The exception is FOLDCNT which is incremented during folded instruction execution.
    ///
    /// Call this method right after `move_pipeline(...)`, so that we know
    /// if any instruction starts being executed in this cycle.
    pub(super) fn handle_dwt_counters(core: &CoreComponent, ctx: &mut Context) {
        // [TI-TRM] 2.7.1 - DWT registers
        let this = Self::component_to_member(core);
        if this.main_slot.as_ref().is_some_and(|c| c.cycle_cntr == 0)
            || this
                .pipelined_slot
                .as_ref()
                .is_some_and(|c| c.cycle_cntr == 0)
        {
            // First cycle, so no special DWT counter is incremented
            //
            // The exception is FOLDCNT counter, which is incremented during
            // folded instruction execution, as we do not know if the instruction
            // won't be skipped (because of branches and conditional execution)
        } else {
            let cycle_counted_towards_lsu_counter = if cfg!(feature = "soc-stm32f100rbt6") {
                this.main_slot
                    .as_ref()
                    .is_some_and(|iectx| iectx.instruction().is_lsu_instruction())
            } else {
                this.most_recent_instruction_uses_lsu
            };

            if cycle_counted_towards_lsu_counter {
                core.dwt.increment_lsu_counter(ctx);
            } else {
                core.dwt.increment_cpi_counter(ctx);
            }
        }
    }

    /// Result of this function may be invalid if the function is called
    /// before moving the pipeline (in cycles where it should be done).
    pub(super) fn get_pc(core: &CoreComponent) -> Word {
        #[cfg(debug_assertions)]
        {
            let this = Self::component_to_member(core);
            this.get_pc_called.set(true);
        }

        // See: [ARM-ARM] B1.4.7
        Self::component_to_member(core).this_instr_addr + 4
    }

    /// [ARM-ARM] B1.5.6 - implementation of `ThisInstrAddr()` in `ReturnAddress()`
    /// pseudocode.
    pub(in crate::component::core) fn this_instr_addr(core: &CoreComponent) -> Word {
        Self::component_to_member(core).this_instr_addr
    }

    /// [ARM-ARM] B1.5.6 - implementation of `NextInstrAddr()` in `ReturnAddress()`
    /// pseudocode.
    pub(super) fn next_instr_addr(core: &CoreComponent) -> Word {
        Self::component_to_member(core).next_instr_addr
    }

    #[allow(clippy::shadow_unrelated)]
    pub(super) fn run_execute(core: &mut CoreComponent, ctx: &mut Context) -> Option<TriggerData> {
        let mut this = Self::get_proxy(core);
        let has_main = this.main_slot.is_some();
        let has_pipelined = this.pipelined_slot.is_some();

        trace!(
            "Execute XPSRs {}: {} pipelined: {}, xpsr: {}, next_xpsr: {}",
            dife(this.main_slot.is_some(), "main".bold(), "main"),
            this.main_slot
                .as_ref()
                .map(|x| x.visible_xpsr.yellow())
                .display_or(|| "--".bright_red()),
            this.pipelined_slot
                .as_ref()
                .map(|x| x.visible_xpsr.cyan())
                .display_or(|| "--".bright_red()),
            RegisterBank::get_xpsr(this.component()),
            this.main_slot
                .as_ref()
                .map(|x| x.next_xpsr.green())
                .display_or(|| "--"),
        );

        // Main slot
        let trigger_data = if let Some(iectx) = &this.main_slot {
            debug_assert!(!iectx.is_finished);
            Self::run_slot(this.component_mut(), ctx, ActiveSlot::Main, has_pipelined)
        } else {
            // No instruction executed in this cycle
            Some(TriggerData::next_instruction(
                RegisterBitmap::new(),
                RegisterBank::get_xpsr(this.component()),
                false,
            ))
        };

        // Pipelined slot
        let pipeline_trigger_data = if let Some(iectx) = &this.pipelined_slot {
            if iectx.is_finished {
                let xpsr = iectx.next_xpsr;

                Some(TriggerData::next_instruction(
                    RegisterBitmap::new(),
                    xpsr,
                    false,
                ))
            } else {
                Self::run_slot(this.component_mut(), ctx, ActiveSlot::Pipelined, false)
            }
        } else {
            None
        };

        if has_main || has_pipelined {
            Self::set_xpsr_if_slot_finished(this.component_mut());
        }

        if this.interruption_state == InterruptStackingOrUnstackingState::None {
            Self::unify_trigger_data(this.component_mut(), trigger_data, pipeline_trigger_data)
        } else {
            Some(TriggerData::IgnoreCurrent)
        }
    }

    fn unify_trigger_data(
        core: &mut CoreComponent,
        main_trigger: Option<TriggerData>,
        pipeline_trigger: Option<TriggerData>,
    ) -> Option<TriggerData> {
        let this = Self::component_to_member_mut(core);
        match (main_trigger, pipeline_trigger) {
            // This shouldn't happen
            (None, None) => None,
            (None, Some(_)) => todo!("Shouldn't happen"),
            (m, None) => m,
            (Some(m), Some(p)) => Some(match (m, p) {
                (TriggerData::IgnoreCurrent, _) | (_, TriggerData::IgnoreCurrent) => {
                    TriggerData::IgnoreCurrent
                }
                (
                    TriggerData::DecodeCurrent {
                        dirty_registers,
                        xpsr: _xpsr,
                        postponed_advance_head,
                    },
                    TriggerData::DecodeCurrent {
                        dirty_registers: pipelined_dirty_registers,
                        xpsr: pipelined_xpsr,
                        postponed_advance_head: pipelined_postponed_advance_head,
                    },
                ) => TriggerData::DecodeCurrent {
                    dirty_registers: (dirty_registers | pipelined_dirty_registers)
                        & !this.pipelined_slot.as_ref().unwrap().written_regs, // Is it actually needed?
                    xpsr: pipelined_xpsr,
                    postponed_advance_head: postponed_advance_head
                        || pipelined_postponed_advance_head,
                },
            }),
        }
    }

    pub(super) fn request_interruption(core: &mut CoreComponent) {
        let this = Self::component_to_member_mut(core);

        if matches!(
            this.interruption_state,
            InterruptStackingOrUnstackingState::None
                | InterruptStackingOrUnstackingState::WaitingForInterrupt
        ) {
            this.interruption_state = InterruptStackingOrUnstackingState::Requested;
        }
    }

    pub(super) fn is_stacking_or_unstacking_running(core: &CoreComponent) -> bool {
        let this = Self::component_to_member(core);
        matches!(
            this.interruption_state,
            InterruptStackingOrUnstackingState::Running
        )
    }

    pub(super) fn restore_execution(core: &mut CoreComponent) {
        let this = Self::component_to_member_mut(core);

        debug_assert!(matches!(
            this.interruption_state,
            InterruptStackingOrUnstackingState::Running
                | InterruptStackingOrUnstackingState::WaitingForInterrupt
        ));

        this.interruption_state = InterruptStackingOrUnstackingState::None;
    }
}

// ============================================================================
// Helper functions
// ============================================================================
#[derive(Debug, PartialEq, Eq, IntoStaticStr, Copy, Clone)]
pub(super) enum PipeliningResult {
    Pipelines,
    NoMainInstr,
    FoldedInstr,
    NotPipelineableAtAll,
    RegisterDependency,
    OperationOnSP,
    UnalignedWriteback,
    PostWriteback,
    AfterStore,
    Writeback,
    RegisterBankConflict,
    Tainted,
    Late,
    NotOnAdvancement,
    Other,
}

impl Execute {
    /// Returns true if execution has been interrupted.
    #[must_use]
    #[allow(clippy::shadow_unrelated)] // For shadowing `this`.
    fn interrupt_execution_if_possible(core: &mut CoreComponent) -> bool {
        let this = Self::component_to_member(core);
        let main_iectx = this
            .main_slot
            .as_ref()
            .expect("Assumption: called only when some instruction executes.");
        debug_assert!(
            main_iectx.folded_instruction().is_none(),
            "Instruction cannot be fold onto multicycle instruction."
        );

        // TODO: Check for other interruptable instructions.
        //       Save their states if necessary.
        //       See [ARM-ARM] B1.5.10 for some of the details.
        let interrupted = match main_iectx.instruction() {
            Instruction::MultiplyAccumulate { .. } | Instruction::MultiplyAndSubtract { .. } => {
                debug_assert!(this.pipelined_slot.is_none());
                true
            }
            // TODO: this should be considered done, so we would return to next_instruction_addr
            // Instruction::WaitForInterrupt => true,
            _ => false,
        };

        if interrupted {
            let this = Self::component_to_member_mut(core);
            this.pipelined_slot = None;
            this.main_slot = None;
            this.last_ctx = None;
            this.active_slot = ActiveSlot::None;
        }

        interrupted
    }

    /// Returns true if `instruction` (with optional folded instruction - `has_folded_instruction`)
    /// can be started in `pipelined_ctx` slot.
    fn can_be_pipelined(
        core: &CoreComponent,
        instruction: &Instruction,
        it_skipped: bool,
        has_folded_instruction: bool,
    ) -> PipeliningResult {
        let this = Self::component_to_member(core);
        let main_ctx = this.get_most_recent_iectx().expect(
            "We can check if instruction is pipelineable only when another one is executing",
        );

        if main_ctx.folded_instruction().is_some() {
            if it_skipped {
                paranoid!(warn, "Skipped instructions were not tested for folding.");
            }
            return PipeliningResult::FoldedInstr;
        }

        // Skipped MLA is not pipelined
        if !it_skipped && Self::can_xmull_mla_be_pipelined(main_ctx, &this.last_ctx, instruction) {
            debug_assert!(
                !has_folded_instruction,
                "MLA is wide instruction, so the folded instruction can't be present",
            );
            return this
                .main_slot
                .is_some()
                .ife(PipeliningResult::Pipelines, PipeliningResult::NoMainInstr);
            // TODO:  hax, true only for normal
        }

        Self::can_lsu_instr_be_pipelined(
            core,
            main_ctx,
            instruction,
            it_skipped,
            has_folded_instruction,
            None,
        )
    }

    #[allow(clippy::many_single_char_names)] // this doesn't work when placed at the closure?
    fn can_lsu_instr_be_pipelined(
        core: &CoreComponent,
        main_ctx: &InstructionExecutionContext,
        next_instruction: &Instruction,
        is_skipped: bool,
        has_folded_instruction: bool,
        #[allow(unused)] ctx: Option<&mut Context>,
    ) -> PipeliningResult {
        // Only on first cycle of data-phase (see tests with stalled multi-cycle ldrs)
        if !main_ctx.instruction().is_lsu_instruction() {
            return PipeliningResult::NotPipelineableAtAll;
        }
        if !LSU::can_pipeline_new_request(core) {
            return PipeliningResult::NotOnAdvancement;
        }

        // [ARM-TRM-G] 18.3 Load-store timings has notes about pipelining, but it is useless
        // How was handled the following errata? -- answ: LDR ..., SP cannot pipeline; STR ..., any! cannot pipeline
        // [ARM-ERR-2008] 377681: Interrupted fault-generating load/store pair with SP base-writeback may corrupt stack
        // The errata concerns a case of ldr/str rA, sp!; ldr/str any and fault is generated e.g. in second cycle
        // and some other interrupts comes as well.

        let pre_desc = main_ctx.instruction().get_memory_description();
        let pre = Self::get_memorier_description(main_ctx.instruction());

        // If the previous one is store, we are delayed by a store buffer. (What was the proof of str/ldr pipelining?)
        // Nevertheless, the waiting is done with a DENY instead of a waitstate.
        // This is seen with unaligned accesses, when we can pipeline only in the cycle-next-after advance).
        // str:   X:DA  X:DD          /- pipelining happens here, so D1 must be first cycle of advancing
        // ldr unalign: X:DA  X:D? X:DD1, X:DD2
        // nop:         D                 X
        // branch:                        D
        // Is it related to?:
        // [ARM-ECM] 463769: Unaligned MPU fault during a write may cause the wrong data to be written to a
        //                   successful first access.
        let is_lsu_unaligned_pipelineable_cycle = LSU::in_unaligned_special(core);
        let pre_is_multiple = pre.data_reg_iter.is_some();
        let pre_is_reg_off = match pre_desc {
            MemoryInstructionDescription::LoadSingle {
                register_offset, ..
            }
            | MemoryInstructionDescription::StoreSingle {
                register_offset, ..
            } => register_offset,
            // note: LDRD is reg off
            _ => false,
        };
        // This is used to determine if a NOP after a finished transfer may still act as a multicycle instr
        let empty_cycles = Self::component_to_member(core).idle_prev_cycles;
        let had_empty_cycles = empty_cycles > 0;

        // todo: this is redundant now
        let is_narrow_tainted = Decode::is_instr_tainted(core);

        // STR (register) only allows consuming actual NOPs (even if RegBank is not used)
        // proof: see `memory/str_reg_pipelining.asm`
        // Yet, it seems that such may pipeline after unaligned STR.
        // See `definitive_lsu_reg_offset-partial12` conf:
        // {'code': 'flash', 'cnt': 'CYCCNT', 'memory': 'sram', 'base_offset': 2, 't': 'ttet',
        //  'flags': 'eq', 'lbEn': True, 'tested_instr': 'str.w {Rt}, [{Rn}, {Rm}]',
        //  'pad1': 'mov.n r6, r5', 'pad2': 'add.n r1, r1', 'stall1': 'x_cyc', 'stall2': 'x_cyc',
        //  'addr_reg': 'r5', 'prev_instr': 'adds.w {dest}, {dest}, {reg_0_or_2}'}
        // Question: what about DENY-stalled STR?
        let is_next_nop = matches!(next_instruction, Instruction::NoOperation);
        let xpsr = main_ctx.visible_xpsr;
        let _is_it_skipped = xpsr.in_it_block() && !xpsr.it_condition_passed();
        let is_non_exec = is_next_nop || is_skipped;
        // definitive_lsu_reg_offset-partial12.1019120
        // it seems that for :ite ok; str sp. [r3]; ldr r6, [sp,4]
        // the skipped ldr can pipeline under the str (in the test the str doesn't inc LSUCNT)
        let is_next_lsu = next_instruction.is_lsu_instruction();
        let is_late = Self::component_to_member(core).main_slot.is_none();
        let is_a_gated_lsu = is_next_lsu && is_late;

        if pre_is_multiple {
            return PipeliningResult::Other;
        }
        let pre_data_reg = pre.single_data_reg.unwrap();
        let pre_data_is_sp = pre_data_reg == RegisterID::SP || pre_data_reg == RegisterID::PC;

        // For STR_register, it seems that:
        // in D: address regs are read
        // in X:A: the target register is read
        // This can mean two things:
        // - if the reg is not fast-forwarded, it must use RegBank
        // - it may conflict with decoding of the following instr (especially if it requires two registers)
        // See: `definitive_lsu_reg_offset-partial12`. With: mem=gpram code
        // isb.w; ldr.w CYC; add.w; add.w; iETTE (pseudo-it); add.n; ldr.w [reg_off]; str.w [reg_off]; pad2
        // This is extra to the reg-dep case and sp-case
        let pre_is_str_reg = pre.is_store && pre_is_reg_off;

        #[cfg(feature = "cycle-debug-logger")]
        #[allow(clippy::cast_lossless)]
        if let Some(ctx) = ctx {
            CycleDebugLoggerProxy.on_free_formatted_u64(
                ctx,
                "nop_pipelining",
                u64::from_le_bytes([
                    is_lsu_unaligned_pipelineable_cycle as u8,
                    empty_cycles,
                    is_late as u8,
                    0,
                    0,
                    is_narrow_tainted as u8,
                    0,
                    0,
                ]),
                |x| {
                    let [a, b, c, d, e, f, ..] = x.to_le_bytes();
                    format!("unal:{a}, empt:{b}, late:{c}, ppsi:{d}, puwr:{e}, T:{f}")
                },
            );
        }

        let pre_is_multiple = pre_is_multiple.then_some(PipeliningResult::Other);
        // Store with writeback pull data in execute, because the second bus is busy for writeback by ALU
        let pre_str_writeback =
            (pre.is_store && pre.writeback).then_some(PipeliningResult::Writeback);
        let pre_data_sp = pre_data_is_sp.then_some(PipeliningResult::OperationOnSP);
        let pre_is_store = pre.is_store.then_some(PipeliningResult::AfterStore);
        let pre_is_str_reg = pre_is_str_reg.then_some(PipeliningResult::RegisterBankConflict);
        let unaligned_str_wb = pre_str_writeback.and(
            is_lsu_unaligned_pipelineable_cycle.then_some(PipeliningResult::UnalignedWriteback),
        );

        let had_empty_cycs_after_finished_str =
            (pre.is_store && is_late && had_empty_cycles).then_some(PipeliningResult::NoMainInstr);
        let is_late =
            had_empty_cycs_after_finished_str.or(is_late.then_some(PipeliningResult::Late));
        let next_non_lsu = (!is_next_lsu).then_some(PipeliningResult::NotPipelineableAtAll);
        let _is_narrow_tainted = is_narrow_tainted.then_some(PipeliningResult::Tainted);
        // phony folded
        let pholded = has_folded_instruction.then_some(PipeliningResult::FoldedInstr);

        let standard_pre_str = pre_str_writeback.or(pre_is_str_reg);
        let standard_pre_ldr =
            (!pre.is_store && pre_data_is_sp).then_some(PipeliningResult::OperationOnSP);

        let (post_not_simple, addr_dependency) = if is_next_lsu {
            // skipped lsu instruction

            let post = Self::get_memorier_description(next_instruction);
            let addr_is_dirty = post
                .addr_registers_bitmap
                .bitand(main_ctx.dirty_regs)
                .count()
                != 0;
            // XXX: someone is going to break this definition, make an explicit helper
            let post_is_multiple = post.data_reg_iter.is_some();
            let post_single_nowb = !post_is_multiple && !post.writeback;

            // [ARM-TRM-G] 18.3 Load-store timings state, that LDR Rx!, [any] cannot be
            // normally pipelined, except for "non register writing instructions": e.g., cmp, tst, nop, skipped it
            // The practice seems to be different and just the writeback register cannot be address source of the next

            // We can distinguish ALU-AGU RAW vs nonpipelining with an unaligned address
            // Apparently the non-pipelining comes from AGU stall and only at A->D transition.
            // For unaligned, the transition happens at the next cycle, so the  address makes to AGU in time.
            let addr_dependency = addr_is_dirty.then_some(PipeliningResult::RegisterDependency);
            let post_not_simple = (!post_single_nowb).then_some(PipeliningResult::PostWriteback);
            (post_not_simple, addr_dependency)
        } else {
            // nop or skipped normal instruction
            (None, None)
        };

        let standard_post = post_not_simple.or(addr_dependency).or(next_non_lsu);
        let mut standard_rules = standard_pre_str.or(standard_pre_ldr).or(standard_post);
        if *cm_hyp::ablation::NO_LDR_POST_STR {
            let ldr_after_str = (pre.is_store
                && matches!(
                    Instruction::get_memory_description(next_instruction),
                    MemoryInstructionDescription::LoadSingle { .. }
                ))
            .then_some(PipeliningResult::AfterStore);
            standard_rules = standard_rules.or(ldr_after_str);
        }

        // Note: without support for "Late", it simplifies a lot
        // And, in principle, for data_sp | ~is_non_exec | _is_narrow_tainted, standard rules are enough.
        pre_is_multiple
            .or(ife(!is_next_nop, pre_is_str_reg, None))
            .or(unaligned_str_wb)
            // .or(ife(!is_a_gated_lsu, standard_post.and(pholded), None))
            .or(ife(!is_a_gated_lsu, standard_rules.and(pholded), None))
            // hax to order Late results -- this is completely redundant
            .or(ife(
                is_a_gated_lsu && is_non_exec,
                pre_is_store.and(standard_post).and(pre_data_sp),
                None,
            ))
            .or(ife(
                is_late.is_some() && is_non_exec,
                pre_is_store.and(next_non_lsu).and(pre_data_sp),
                None,
            ))
            // .or(ife(
            //     is_non_exec && !is_a_gated_lsu,
            //     standard_rules.and(_is_narrow_tainted),
            //     None,
            // ))
            .or(ife(
                is_non_exec && !is_next_nop,
                pre_is_store.and(is_late),
                None,
            ))
            .or(ife(pre_data_is_sp || !is_non_exec, standard_rules, None))
            .unwrap_or(PipeliningResult::Pipelines)
    }

    #[allow(clippy::ref_option)]
    #[allow(clippy::single_match)]
    /// Returns true if `main_ctx.instruction()` and `next_instruction` are `xMULL` and `MLA`
    /// with register dependency and `xMULL` can pipeline with the `MLA` during this cycle.
    fn can_xmull_mla_be_pipelined(
        main_ctx: &InstructionExecutionContext,
        // Just done
        done_ctx: &Option<InstructionExecutionContext>,
        next_instruction: &Instruction,
    ) -> bool {
        match main_ctx.instruction() {
            Instruction::UnsignedMultiplyLong { rd_hi, .. }
            | Instruction::SignedMultiplyLong { rd_hi, .. } => match next_instruction {
                Instruction::MultiplyAccumulate { rn, rm, ra, .. } => {
                    let does_reg_dep_allow_pipelining = rd_hi == ra && rd_hi != rn && rd_hi != rm;
                    // The instruction in xMULL, so we can unwrap its state.
                    let state = main_ctx.state.unwrap_multiply_long_state();
                    let can_xmull_be_pipelined = main_ctx.cycle_cntr == state.write_hi_cycle_no;
                    return does_reg_dep_allow_pipelining && can_xmull_be_pipelined;
                }
                _ => (),
            },
            _ => (),
        }
        // There may be another MLA pipelined when xMULL just finished
        // See data_processing/xmull_mla.asm for an example showing this interaction.
        // It seems that at most two MLAs may be pipelined, and the easiest explanation
        // would be that xMULL *just* finished.
        // TODO: we need large-tests to cover all register options and check what happens
        //       when the first MLA was *late*.
        match (
            done_ctx.as_ref().map(|x| x.instruction()),
            main_ctx.instruction(),
        ) {
            (
                Some(
                    Instruction::UnsignedMultiplyLong { .. }
                    | Instruction::SignedMultiplyLong { .. },
                ),
                Instruction::MultiplyAccumulate { rd, .. },
            ) if main_ctx.was_pipelined && *cm_hyp::ablation::UMULL_MLA_MLA => {
                match next_instruction {
                    Instruction::MultiplyAccumulate { rn, rm, ra, .. }
                        if ra == rd && ra != rn && ra != rm =>
                    {
                        return !main_ctx.is_finished;
                    }
                    _ => (),
                }
            }
            _ => (),
        }
        false
    }

    #[allow(clippy::shadow_unrelated)]
    fn run_slot(
        core: &mut CoreComponent,
        ctx: &mut Context,
        slot: ActiveSlot,
        next_executing: bool,
    ) -> Option<TriggerData> {
        // Prepare context & pc
        let this = Self::component_to_member_mut(core);
        this.active_slot = slot;

        let iectx = this.get_active_instruction_execution_context();

        let following_instruction_address: Word = iectx.following_instruction_address().into();
        let seen_pc = Self::get_pc(core);
        let pc_ctx = RegisterBank::with_pc(core, seen_pc);

        // Execute
        let exec_result = Self::execute_instruction_step(core, ctx);
        if exec_result.runs_next_instruction() {
            Self::execute_folded_instruction(core, ctx);
        }

        debug_assert!(
            next_executing.implies(!exec_result.does_branch()),
            "We cannot branch while the next instruction is already in execution"
        );

        let trigger_data =
            Self::handle_exec_result(core, &exec_result, following_instruction_address);

        // Cleanup
        let this = Self::component_to_member_mut(core);
        this.active_slot = ActiveSlot::None;
        pc_ctx.release(core);

        trigger_data
    }

    #[allow(clippy::shadow_unrelated)] // `this` shadowing
    fn handle_exec_result(
        core: &mut CoreComponent,
        exec_result: &ExecutionStepResult,
        following_instruction_address: Word,
    ) -> Option<TriggerData> {
        let this = Self::component_to_member_mut(core);
        let iectx = this.get_active_instruction_execution_context();
        // Now each slot returns information only concerning it
        let dirty_registers = iectx.dirty_regs;
        let next_xpsr = iectx.next_xpsr;
        // TODO: work on this skipped branches (LDR PC too?) are postponing adancement?
        // r.n. iectx.instruction() returns nop for skipped (so the second part doesn't make sense)
        let postponed_advanced_head = iectx.instruction().does_postpone_advance_head()
            && !(iectx.visible_xpsr.in_it_block()
                && matches!(exec_result, ExecutionStepResult::Skipped)
                && !iectx.instruction().is_branch());
        match exec_result {
            ExecutionStepResult::Continue {
                trigger_decode,
                lsu_branch_expected,
            } => {
                if *trigger_decode {
                    if *lsu_branch_expected {
                        Some(TriggerData::ignore_currently_decoding_instruction())
                    } else {
                        Some(TriggerData::next_instruction(
                            dirty_registers,
                            next_xpsr,
                            postponed_advanced_head,
                        ))
                    }
                } else {
                    None
                }
            }
            ExecutionStepResult::NextInstruction => {
                iectx.mark_last_cycle();
                this.next_instr_addr = following_instruction_address;
                Some(TriggerData::next_instruction(
                    dirty_registers, // Skipped instructions are not visible to Execute
                    next_xpsr,
                    postponed_advanced_head,
                ))
            }
            ExecutionStepResult::Skipped | ExecutionStepResult::BranchNotTaken => {
                iectx.mark_last_cycle();
                this.next_instr_addr = following_instruction_address;
                Some(TriggerData::next_instruction(
                    RegisterBitmap::new(), // Skipped instructions are not visible to Execute
                    next_xpsr,
                    postponed_advanced_head,
                ))
            }
            ExecutionStepResult::ExecuteTimeBranch { address } => {
                iectx.mark_last_cycle();
                this.next_instr_addr = *address;
                Fetch::make_branch(core, *address);
                Some(TriggerData::ignore_currently_decoding_instruction())
            }
            ExecutionStepResult::LateBranch | ExecutionStepResult::ExceptionReturn => {
                iectx.mark_last_cycle();
                Fetch::disable_fetch(core);
                Some(TriggerData::ignore_currently_decoding_instruction())
            }
            ExecutionStepResult::DecodeTimeBranch {
                address,
                was_speculative: _was_speculative,
            } => {
                iectx.mark_last_cycle();
                this.next_instr_addr = address.with_bit_set(0, false);
                Some(TriggerData::ignore_currently_decoding_instruction())
            }
            ExecutionStepResult::PreSleep => {
                Some(TriggerData::ignore_currently_decoding_instruction())
            }
            ExecutionStepResult::Sleep => {
                iectx.mark_last_cycle();
                this.next_instr_addr = following_instruction_address;
                if this.interruption_state == InterruptStackingOrUnstackingState::None {
                    this.interruption_state =
                        InterruptStackingOrUnstackingState::WaitingForInterrupt;
                }
                Some(TriggerData::ignore_currently_decoding_instruction())
            }
        }
    }

    /// We hold an invariant:
    /// After execution of an instruction, resulting [`XPSR`] is being
    /// set in the [`RegisterBank`]. Furthermore, the setting occurs
    /// in proper sequence, i.e., main instruction must set the [`XPSR`]
    /// before pipelined one.
    ///
    /// Note: pipelined instruction "sees" modifications from main
    /// instruction, thus setting resulting [`XPSR`] from pipelined
    /// inustruction, commits changes made by main instruction.
    /// See [`Execute::move_pipeline`] for implementation.
    fn set_xpsr_if_slot_finished(core: &mut CoreComponent) {
        let this = Self::component_to_member(core);
        match (&this.main_slot, &this.pipelined_slot) {
            (
                Some(InstructionExecutionContext {
                    is_finished: true,
                    next_xpsr,
                    ..
                }),
                None,
            )
            | (
                Some(InstructionExecutionContext {
                    is_finished: true, ..
                }),
                Some(InstructionExecutionContext {
                    is_finished: true,
                    next_xpsr,
                    ..
                }),
            ) => {
                let xpsr = *next_xpsr;
                RegisterBank::set_xpsr(core, xpsr);
            }
            (None, _) => panic!("This method should not be called if no instruction is being run."),
            _ => (),
        }
    }

    fn finish_instruction_in_tock(core: &mut CoreComponent, clear_reg: Option<RegisterID>) {
        // LSU instructions may be finished late in the cycle
        // Fortunately, we always pass trigger_decode=true
        // Probably only XPSR is not updated properly
        let this = Self::component_to_member_mut(core);
        let iectx = this
            .main_slot
            .as_mut()
            .expect("Late finishing instruction can be only made in the in main slot");
        debug_assert!(matches!(
            iectx.state,
            InstructionExecutionState::MultipleLoadStore(_)
                | InstructionExecutionState::SingleLoadStore(_)
        ));
        if let Some(clear_reg) = clear_reg {
            iectx.mark_register_clean(clear_reg);
        }
        if clear_reg != Some(RegisterID::PC) {
            let following_instruction_address: Word = iectx.following_instruction_address().into();
            this.next_instr_addr = following_instruction_address;
        }
        iectx.mark_last_cycle();
        // TODO: test if forwarding the XPSR works with LDR in last it and next b.n (undefined in it)
        Self::set_xpsr_if_slot_finished(core);
    }

    #[track_caller]
    fn get_active_instruction_execution_context(&mut self) -> &mut InstructionExecutionContext {
        match self.active_slot {
            ActiveSlot::None => {
                panic!("Cannot get active instruction execution context if no slot is active")
            }
            ActiveSlot::Pipelined => self
                .pipelined_slot
                .as_mut()
                .expect("Active slot should have some instruction execution context"),
            ActiveSlot::Main => self
                .main_slot
                .as_mut()
                .expect("Active slot should have some instruction execution context"),
        }
    }

    #[allow(dead_code)] // Leave the impl for a while
    fn get_dirty_registers(&self) -> RegisterBitmap {
        let empty_bitmap = RegisterBitmap::new(); // 16-bit number, so constructing it beforehand is ok
        let main_dirty = self
            .main_slot
            .as_ref()
            .map_or(empty_bitmap, |c| c.dirty_regs);
        let (pipelined_dirty, pipelined_written) = self
            .pipelined_slot
            .as_ref()
            .map_or((empty_bitmap, empty_bitmap), |c| {
                (c.dirty_regs, c.written_regs)
            });

        (main_dirty | pipelined_dirty) & !pipelined_written
    }
}

impl InstructionExecutionContext {
    fn new(pipeline_step_pack: PipelineStepPack, xpsr: XPSR) -> Self {
        let skipped = xpsr.in_it_block()
            && !xpsr.it_condition_passed()
            && !pipeline_step_pack
                .instruction
                .is_unconditional_in_it_block();
        let ignored_branch = pipeline_step_pack.branch_kind == Brchstat::NoBranch
            && pipeline_step_pack.instruction.is_branch();

        let dirty_registers = if !skipped && !ignored_branch {
            pipeline_step_pack.instruction.get_written_registers()
        } else {
            RegisterBitmap::new()
        };
        Self {
            pipeline_step_pack,

            state: InstructionExecutionState::None,

            cycle_cntr: 0,

            dirty_regs: dirty_registers,
            next_dirty_regs: dirty_registers,

            written_regs: RegisterBitmap::new(),
            next_written_regs: RegisterBitmap::new(),

            is_finished: false,
            was_pipelined: false,

            visible_xpsr: xpsr,
            next_xpsr: xpsr,

            #[cfg(debug_assertions)]
            apsr_modified: false,
            #[cfg(debug_assertions)]
            epsr_modified: false,
            #[cfg(debug_assertions)]
            folded_instr_executed: false,
        }
    }

    fn is_skipped(&self) -> bool {
        self.visible_xpsr.in_it_block()
            && !self.visible_xpsr.it_condition_passed()
            && !self.raw_instruction().is_unconditional_in_it_block()
    }
    fn raw_instruction(&self) -> &Instruction {
        &self.pipeline_step_pack.instruction
    }
    fn effective_instruction(&self) -> &Instruction {
        if self.is_skipped() {
            &Instruction::NoOperation
        } else {
            &self.pipeline_step_pack.instruction
        }
    }

    fn instruction(&self) -> &Instruction {
        self.effective_instruction()
    }

    fn instruction_address(&self) -> Address {
        self.pipeline_step_pack.address
    }

    fn has_folded_instruction(&self) -> bool {
        self.pipeline_step_pack.folded_instruction.is_some()
    }

    fn folded_instruction(&self) -> Option<&Instruction> {
        self.pipeline_step_pack.folded_instruction.as_ref()
    }

    fn folded_instruction_address(&self) -> Address {
        self.pipeline_step_pack
            .folded_instruction_address()
            .expect("There has to be some folded instruction executed")
    }

    fn following_instruction_address(&self) -> Address {
        self.pipeline_step_pack.following_instruction_address()
    }

    fn mark_register_clean(&mut self, register: RegisterID) {
        // register should marked clean only once
        debug_assert!(self.next_dirty_regs.get(register));
        debug_assert!(!self.next_written_regs.get(register));
        self.next_dirty_regs = self.next_dirty_regs.with(register, false);
        self.next_written_regs = self.next_written_regs.with(register, true);
    }

    fn mark_all_registers_clean(&mut self) {
        self.next_dirty_regs = RegisterBitmap::new();
        // Do not update `(next_)written_regs`: no register has been written.
    }

    fn mark_last_cycle(&mut self) {
        debug_assert!(self.next_dirty_regs.are_all_cleared());
        debug_assert!(!self.is_finished);
        self.is_finished = true;
    }

    fn mark_pipelined(&mut self) {
        debug_assert!(!self.is_finished);
        self.was_pipelined = true;
    }

    fn modify_apsr<F>(&mut self, f: F)
    where
        F: FnOnce(XPSR) -> XPSR,
    {
        #[cfg(debug_assertions)]
        {
            assert!(!self.apsr_modified);
            self.apsr_modified = true;
        }
        debug_assert!(
            self.cycle_cntr == 0,
            "Assumption: xPSR is modified in the first cycle what simplifies xPSR fast forwarding to decode"
        );

        self.next_xpsr = self.next_xpsr.with_modified_apsr(f);
    }

    fn modify_epsr<F>(&mut self, f: F)
    where
        F: FnOnce(XPSR) -> XPSR,
    {
        #[cfg(debug_assertions)]
        {
            assert!(!self.epsr_modified);
            self.epsr_modified = true;
        }
        debug_assert!(
            self.cycle_cntr == 0,
            "Assumption: xPSR is modified in the first cycle what simplifies xPSR fast forwarding to decode"
        );

        self.next_xpsr = self.next_xpsr.with_modified_epsr(f);
    }
}

impl InstructionExecutionState {
    #[track_caller]
    fn unwrap_multiply_long_state(&self) -> &MultiplyLongExecutionState {
        if let Self::MultiplyLong(state) = self {
            state
        } else {
            panic!("Expected `InstructionExecutionState::MultiplyLong(_)`, but got: {self:?}");
        }
    }

    #[track_caller]
    #[allow(dead_code)]
    fn unwrap_single_load_store_state(&self) -> &SingleLoadStoreExecutionState {
        if let Self::SingleLoadStore(state) = self {
            state
        } else {
            panic!("Expected `InstructionExecutionState::SingleLoadStore(_)`, but got: {self:?}");
        }
    }

    #[track_caller]
    #[allow(dead_code)]
    fn unwrap_single_load_store_state_mut(&mut self) -> &mut SingleLoadStoreExecutionState {
        if let Self::SingleLoadStore(state) = self {
            state
        } else {
            panic!("Expected `InstructionExecutionState::SingleLoadStore(_)`, but got: {self:?}");
        }
    }

    #[track_caller]
    fn unwrap_multiple_load_store_state_mut(&mut self) -> &mut MultipleLoadStoreExecutionState {
        if let Self::MultipleLoadStore(state) = self {
            state
        } else {
            panic!("Expected `InstructionExecutionState::MultipleLoadStore(_), but got {self:?}");
        }
    }
}
